Binary, Multiple, and Triple Class Dataset (Predictive Analytics)

Description: The capstone project, titled 'Cyber Threat Intelligence Sharing by Implementing Federated Learning (FL) and Homomorphic Encryption (HME) to Protect Privacy of Data,' is my final year project for Master's degree in Information Technology at Griffith University. My role involved delivering the code, constituting 60% of the deliverables to the client. To train the Artificial Neural Network (ANN) model for predicting probable classes of cyber attacks, I utilized three datasets: 1) Binary with two classes ('Attack' or 'Natural'), 2) Triple with three classes ('Attack,' 'Natural,' and 'No Events'), and 3) Multi with 37 different numeric classes. Federated learning enables client organizations to share model parameters after being trained on local data, protecting the confidentiality of data while allowing the sharing of experiences among organizations. To enhance data protection, homomorphic encryption encrypts model parameters before transporting them to the global server for further iterations.The major challenges faced in code development included understanding the concepts of FL and HME, selecting a Python library for FL, and improving accuracy metrics in predicting target classes. TensorFlow was used for FL, and the TenSEAL library was employed for HME. Due to client confidentiality and ownership rights, I have only provided the part of code, project report, and slides from my first-wave award presentation in the above folder.
                                                                    
Car and Bike Sales Dataset (Excel and IBM Cognos)

Description: This personal project focused on analyzing data to provide insights for car and bike sales departments. Leveraging the pivot table functionality in Excel, I created charts and visualized them in dashboards within Excel and IBM Cognos.

Chicago Dataset (Python, SQL Magic, and SQLite)

Description: This personal project involved exploring various datasets related to Chicago city, including census data, crime statistics, and information about public schools. I used the Pandas library to extract data into data frames and employed SQL Magic queries to retrieve information from an SQLite database. SQL operations such as GROUP BY, ORDER BY, LIKE, MAX, and MIN were utilized.

Covid19 Dataset (MySQL and Tableau)

Description: This project aimed at gaining experience in MySQL and Tableau visualization. I performed SQL queries to extract information from two datasets, namely covid_deaths and covid_vaccinations. In addition to descriptive SQL queries, I utilized advanced concepts like CTE, Temporary table, and View statements. The Tableau Public platform was employed to create dashboards, and the visualization can be viewed at this URL: https://public.tableau.com/app/profile/adnan.chowdhry2764/viz/portfolio_wb/Dashboard1?publish=yes.

SEEK Job Market Dataset (Python, Pandas, NumPy, Matplotlib, Seaborn)

Description: This significant project played a crucial role in enhancing my data analytics skills during my final year of studies in the Master of Information Technology program. Working with a substantial dataset of over 300k samples from SEEK Job Market, I applied various Python libraries for tasks such as connecting to data, describing the data, cleaning the data, understanding the data, analyzing the data, and visualizing the data. The skills acquired during this project include proficiency in Python, Pandas, NumPy, Matplotlib, and Seaborn.

Company Employee Dataset ( MS Excel )

Description: This project aimed at cleaning, analyzing and visualizing data of employees from an organization which is based in India and Newzealand. After performing more than 25 operations on data I have enhanced my skills in transforming data using Power Query, data analysis using Pivot tables and formulas, and presenting insights using charts.

ETL Implimentation using Power Query ( MS Excel )

Description: This project utilized the process of extract, transform and load (ETL) data from multiple sources in Power Query to make connections to data, Shape data to meet the needs for analysis while the original source remains unchanged, Integrate data from multiple sources to get a unique view into the data, Complete the query, load it into a worksheet or Data Model and periodically refresh it. 




