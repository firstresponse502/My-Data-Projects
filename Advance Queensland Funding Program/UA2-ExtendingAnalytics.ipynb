{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef62cbd-2ea0-442e-973e-e50ca1e97fa2",
   "metadata": {
    "cell_name": "header_cell"
   },
   "source": [
    "# <div style=\"background:#FFFFEE; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers (2024 Sem 1)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff597d6e-cdac-4d1a-b657-f6d756380ae3",
   "metadata": {
    "cell_name": "title_cell"
   },
   "source": [
    "# IFN619 :: UA2 - Extending Analytics (40%)\n",
    "\n",
    "**IMPORTANT:** Refer to the instructions in Canvas [UA2 - Assignment 2 - extending analytics](https://canvas.qut.edu.au/courses/17432/assignments/163774) *BEFORE* working on this assignment.\n",
    "\n",
    "#### REQUIREMENTS ####\n",
    "\n",
    "1. Complete and run the code cell below to display your name, student number, and assignment option\n",
    "2. Identify an appropriate question (or questions) to be addressed by your overall data analytics narrative\n",
    "3. Extend your analysis in assignment 1 with:\n",
    "    - the analysis of additional unstructured data using the Guardian API (See accessing the Guardian API notebook),\n",
    "    - the use of one machine learning technique (as used in the class materials), and\n",
    "    - identification of ethical considerations relevant to the analysis (by drawing on class materials).\n",
    "4. Ensure that you include documentation of your thinking and decision-making using markdown cells\n",
    "5. Ensure that you include appropriate visualisations, and that they support the overall narrative\n",
    "6. Ensure that your insights answer your question/s and are appropriate to your narrative. \n",
    "7. Ensure that your insights are consistent with the ethical considerations identified.\n",
    "\n",
    "**NOTE:** you should not repeat the analysis from assignment 1, but you may need to save dataframes from assignment 1 and reload for use in this assignment. You may also summarise your assignment 1 insights as part of the process of identifying questions for analysis.\n",
    "\n",
    "#### SUBMISSION ####\n",
    "\n",
    "1. Create an assignment 2 folder named in the form **UA2-surname-idnumber** and put your notebook and any data files inside this folder. Note, do not put large training data in this folder (reference any training data that you used but keep it outside this folder), only keep small data files and models in this folder with your notebook.\n",
    "2. When you have everything in the correct folder, reset all cells and restart the kernel, then run the notebook completely, checking that all cells have run without error. If you encounter errors, fix your notebook and re-run the process. It is important that your notebook runs without errors only requiring the files in the folder that you have created.\n",
    "3. When the notebook is error free, zip the entire folder (you can select download folder in Jupyter).\n",
    "4. Submit the zipped folder on Canvas [UA2 - Assignment 2 - extending analytics](https://canvas.qut.edu.au/courses/17432/assignments/163774)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763f6f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Adnan Chowdhry (11869828)</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the following cell with your details and run to produce your personalised header for this assignment\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# personal detail\n",
    "first_name = \"Adnan\"\n",
    "last_name = \"Chowdhry\"\n",
    "student_number = 11869828\n",
    "\n",
    "personal_header = f\"<h1>{first_name} {last_name} ({student_number})</h1>\"\n",
    "HTML(personal_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbcf4f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31d63d-aa89-4329-9594-5cc3c28d172a",
   "metadata": {},
   "source": [
    "## CONTEXT\n",
    "In Assignment 1, I established that there is an imbalance in Advance Queensland funds distribution between SEQ and non-SEQ regions. However, this imbalance is not due to population differences or a bias towards SEQ recipients. Instead, it fulfills the clear objective of promoting science, technology, and innovation initiatives. It is merely a coincidence that the majority of significant amounts are awarded to SEQ clients. Additionally, I found that the overall committed amounts have declined from 2019 to 2023. One reason for this decline could be that, after funding notable and worthy clients throughout the state, proposals for less impactful projects are now being submitted. This indicates that Advance Queensland needs to take the lead and collaborate with public and private sector organizations to initiate projects that require urgent attention.\n",
    "\n",
    "\n",
    "## INTRODUCTION\n",
    "In 2020, the funding commitment of $5 million to AI Consortium Pty Ltd for the AI Hub seemed appropriate and future-oriented. However, from then until 2023, no further monetary investments have been made in the broad AI domain except some specific projects such as in health or mininng. This suggests an underestimation of AI's potential to transform the future of technology. The lack of funding to either support the positive aspects or mitigate the negatives of AI is particularly concerning given the arrival of Generative AI applications such as ChatGPT in 2023, which have sparked intense debates about the risks for users and regulators. The focus of my analysis is to give a glance how the world is reacting to this technology and what are the risks this AI technology poses to the society.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc619991-7e52-4efb-8fb2-dff1aefe3612",
   "metadata": {},
   "source": [
    "## QUESTION 1: \n",
    "What projects in the domain of AI has Advance Queensland funded, and which of them have the potential to address its broader challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffbffb6-3f62-42a3-a767-5a12e9a3f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee8965-233c-431b-a8fd-666f38840276",
   "metadata": {},
   "source": [
    "## ADVANCE QLD FUNDING RECIPIENTS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28db30fd-f65c-419e-91f4-7df94d9fd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a CSV file into a new dataframe from a url\n",
    "funding_recipients_data = \"https://www.data.qld.gov.au/dataset/db190f2d-f866-4811-9a6e-4b78744b551b/resource/0f97b985-f5c7-49d2-8b0a-bc5dfbe070b9/download/advance-queensland-funding-recipients.csv\"\n",
    "\n",
    "df = pd.read_csv(funding_recipients_data, encoding='latin 1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b1e04-4df7-431a-9822-8183bbd61d54",
   "metadata": {},
   "source": [
    "## Ethical Considerations in Data Access and Use:\n",
    "The Advance Queensland funding recipients data is freely available for sharing and adaptation without the need for anonymization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3227c-7cb3-4b48-b6b0-81e2433fa4dd",
   "metadata": {},
   "source": [
    "In this step, I will remove the inconsistencies from \"Actual Contractual Commitment\" column and convert it to data type float which would allow me to use it in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2f7736-4c59-47cf-849f-cb2437174ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Actual Contractual Commitment ($)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Actual Contractual Commitment ($)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Chat gpt helped me with the loop used in this code\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# to get the aggregate of the column 'Actual Contractual Commitment ($)' we need to remove the inconsistencies and convert it to float\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Loop through each value in the 'amount' column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActual Contractual Commitment ($)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Check if value contains a comma\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Replace comma with empty string\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         df\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Contractual Commitment ($)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Actual Contractual Commitment ($)'"
     ]
    }
   ],
   "source": [
    "# Chat gpt helped me with the loop used in this code\n",
    "# to get the aggregate of the column 'Actual Contractual Commitment ($)' we need to remove the inconsistencies and convert it to float\n",
    "\n",
    "# Loop through each value in the 'amount' column\n",
    "for idx, value in enumerate(df['Actual Contractual Commitment ($)']):\n",
    "    # Check if value contains a comma\n",
    "    if ',' in value:\n",
    "        # Replace comma with empty string\n",
    "        df.loc[idx, 'Actual Contractual Commitment ($)'] = value.replace(',', '')\n",
    "\n",
    "# Convert column to float\n",
    "df['Actual Contractual Commitment ($)'] = df['Actual Contractual Commitment ($)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d35e4-20d4-4f54-b8c2-a4dea08b2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c64aeb-b6d2-4b50-9363-f6efce223b58",
   "metadata": {},
   "source": [
    "In this step, I would like to confirm whether the data type has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e6230-6d66-4d8b-8217-f3f0bd76b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing data types of columns \n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a05d33-6c98-4bd0-8636-9eb29b342bfc",
   "metadata": {},
   "source": [
    "In this step, I am converting the 'Approval date' column to datetime format, considering the possibility of using it for comparison and extraction of data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d844f-efe7-42d0-a9bd-35ee9f1fc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_column' from object to datetime\n",
    "df['Approval date'] = pd.to_datetime(df['Approval date'], format='%d/%m/%Y')\n",
    "\n",
    "# Check the data types after conversion\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b6a8b-27e2-49b3-b4be-0efb650bffda",
   "metadata": {},
   "source": [
    "In this step, I would like to see the first and the last day of data to make sense of some of the dates I will consider in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a69c6-1e88-4cd5-a11c-9c449b1dc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first and last dates in the 'Approval date' column\n",
    "first_date = df['Approval date'].min()\n",
    "last_date = df['Approval date'].max()\n",
    "\n",
    "print(\"First date:\", first_date)\n",
    "print(\"Last date:\", last_date, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56636b56-233e-436f-a0b0-b17f3976e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date_column in descending order\n",
    "df_sorted = df.sort_values(by='Approval date', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77665beb-ffd6-47fc-b0b3-c13fb038c12d",
   "metadata": {},
   "source": [
    "In this step, I am extracting the year part from the 'Approval date' to create a new column to use it in my analysis for grouping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc36a87-a6a5-4405-9f3f-de37dda696d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from 'Approval date' and create a new column\n",
    "df_sorted.loc[:, 'Year'] = df_sorted['Approval date'].dt.year\n",
    "\n",
    "# listing the column names\n",
    "column_names = df_sorted.columns\n",
    "print('List of column Names: \\n')\n",
    "for name in column_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832e8eb-0fa9-4752-9150-ca1c8b4ec1cf",
   "metadata": {},
   "source": [
    "In this step, I am pulling all the rows in which the project titles contain the string \"artificial intelligence,\" regardless of case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ee060-6e06-4ff9-a5ac-907713a2206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows containing the string \"Artificial Intelligence\" in the 'Investment/Project Title' column (case insensitive)\n",
    "filtered_df = df_sorted[df_sorted['Investment/Project Title'].str.contains('artificial intelligence', case=False)]\n",
    "\n",
    "# Displaying the number of filtered DataFrame rows and columns\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6074a9-c38a-4ae6-84e5-89dd04209987",
   "metadata": {},
   "source": [
    "In this step, I would like to display the project titles I have extracted above in a list to understand the objective and scope of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b90c30-6550-443c-9689-324ba082d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store formatted titles\n",
    "formatted_titles = []\n",
    "\n",
    "# Loop through the column and store formatted titles in the list\n",
    "for title in filtered_df['Investment/Project Title']:\n",
    "    #formatted_title = '\\n'.join(textwrap.wrap(title, width=80))\n",
    "    formatted_titles.append(title)\n",
    "\n",
    "# Print each formatted title from the list\n",
    "for title_list in formatted_titles:\n",
    "    print(title_list)\n",
    "    print('-' * 80)  # Separator line for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96825d-c397-4e38-95fd-93cc96d5b95b",
   "metadata": {},
   "source": [
    "## INSIGHT:\n",
    "As we can see, out of 10 projects, only 2 seem to have broader goals beyond solving a specific existing problem. The first is the Artificial Intelligence Hub, and the second is the Artificial Intelligence STEM Platform. The AI STEM Platform is focused on the education sector, whereas the AI Hub aims to connect, promote, and enhance AI capabilities. The AI Hub is a project that brings together businesses, government, and research institutes to address current and future challenges of AI with support and accountability\n",
    "(https://qldaihub.com/about-2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37b67b-560e-471a-8917-b3995bbeb6b9",
   "metadata": {},
   "source": [
    "In this step, I would like to see the details of the funding to AI Consortium Pty Ltd to know the stakeholders and the amount in hand for spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04c7de-c4f6-49cc-b706-97e97bf6bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows belonging to the program \"Artificial Intelligence Hub\"\n",
    "AIhub_df = filtered_df[filtered_df['Investment/Project Title'] == 'Artificial Intelligence Hub']\n",
    "AIhub_df = AIhub_df[['Year', 'Investment/Project Title', 'Recipient Name', 'University Collaborator (if applicable)','Other Partners; Collaborators (if applicable)', 'Actual Contractual Commitment ($)']]\n",
    "AIhub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a186a-7fd3-4ca1-be50-2c208a3c88cd",
   "metadata": {},
   "source": [
    "## INSIGHT:\n",
    "I would call this project a holistic endeavor where all the bases are covered: great purpose, partners, and financial resources. However, the objectives and funding should be revised to address future challenges, or new projects should be initiated. This has not happened, as the AI Hub remains the only project of its kind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec4afd-ca74-4d89-ac3f-dcca4d782894",
   "metadata": {},
   "source": [
    "## QUESTION 2:\n",
    "At this point in my analysis, I want to highlight how power-hungry tech companies are reacting to the arrival of OpenAI's ChatGPT application. This reaction underscores the intense challenges that users, regulators, and lawmakers face in handling AI. These challenges are likely to become even more significant, requiring consistent effort to survive and thrive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc38c2-250d-4c29-9402-d70960ff2555",
   "metadata": {},
   "source": [
    "## UNSTRUCTURED DATA FROM GUARDIAN OPEN PLATFORM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16994046-8952-481f-a940-bc64b7b62d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - articles from The Guardian about the war in Ukraine\n",
    "file_path = \"\"\n",
    "file_name = \"microsoft_invest_articles.json\"\n",
    "\n",
    "with open(f\"{file_path}{file_name}\",'r', encoding='utf-8') as fp:\n",
    "    articles = json.load(fp)\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles from {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc74f96-eb62-4782-8228-a973d0bd4c12",
   "metadata": {},
   "source": [
    "## Ethical Considerations in Data Access and Use:\n",
    "The Guardian news data is available for free use in any non-profit project. Access to this data has been granted following registration and the acquisition of a developer key. All necessary guidelines and protocols provided by The Guardian for accessing and using the data have been followed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201f27a-3e83-47a6-ad1a-152ec1efc1ad",
   "metadata": {},
   "source": [
    "In this step, I would show article titles which contains ChatGPT and OpenAI keywords in the year 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870355c8-27d8-4b97-b30f-e7380e09cdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = [\"chatgpt\", \"openai\"]\n",
    "\n",
    "for i, title in enumerate(articles.keys()):\n",
    "    if any(keyword.lower() in title.lower() for keyword in keywords):\n",
    "        print(f\"Title {i+1}: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bed86a-d1ae-4692-b48a-cb103af4be9c",
   "metadata": {},
   "source": [
    "## INSIGHT:\n",
    "The above output shows that number of tech giants such as Microsoft in Title 3 and Amazon in Title 6 investing billions into AI which confirms the importance of AI technology and the potential to draw profits for these organizations. On the other hand, one can also understand the speed with which this technology will evolve because of the scale of investment. To conclude I would like to say that embracing this technology in our society would not only bring the ease but also challenge everone of us at different levels that is why there is a need to monitor and improvise with urgency to gain positive impact from it. \n",
    "\n",
    "## Ethical Consideration:\n",
    "Asserting that major tech companies high investments in AI-driven projects, such as ChatGPT, are driving a significant shift in technology is not necessarily baseless or negative publicity for these organizations. Rather, it reflects a trend observed in multiple articles included in this analysis, indicating that numerous knowledgeable individuals share the same perspective. For instance, titles such as \"Title 4:UK watchdog to examine Microsoft’s partnership with OpenAI\" and \"Title 31:The OpenAI meltdown will only accelerate the artificial intelligence race\" by Sarah Kreps highlight the growing attention and scrutiny surrounding AI advancements and collaborations.To justify the heavy investments in AI it is critical for stakeholders to balance the rapid advancement and integration of AI technology with the potential societal impacts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ab032-0c3e-4ac3-b5e7-a126f20e6032",
   "metadata": {},
   "source": [
    "## QUESTIONS 3:\n",
    "What are the risks from the use of AI technology such as ChatGPT if the users, regulators and the law makers do not take appropriate and timely actions to control the spread of this technology into the society?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1826a-7e2c-48ca-b810-1d35f61cf5ee",
   "metadata": {},
   "source": [
    "## UNSTRUCTURED DATA FROM GUARDIAN OPEN PLATFORM API\n",
    "To answer the above question, I have chosen to use the guardian open platform to see the news articles available regarding this issue and by the use of tfidf and nmf topic modelling technique I would extract latent topics on risks of AI within a corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f59f86-3cad-443d-a194-0dabfa2b3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - articles from The Guardian about the war in Ukraine\n",
    "file_path = \"\"\n",
    "file_name = \"AI risks to society.json\"\n",
    "\n",
    "with open(f\"{file_path}{file_name}\",'r', encoding='utf-8') as fp:\n",
    "    documents = json.load(fp)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020de9ea-3f8f-424e-b98f-438d05349774",
   "metadata": {},
   "source": [
    "In this step, I will convert a collection of documents which I extracted from guardian open platform and saved in Json file in to a matrix of tfidf features(terms). I used tfidf because tfidf provides an effective way to preprocess text data for nmf such as normalization and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5859f-bf71-447e-846d-88a2b1f24909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only count terms that in maximum of 75% of documents, and a minimum of 2 documents. \n",
    "# Count a maximum of 10000 terms, and remove common english stop words\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.75, min_df=2, max_features=10000, stop_words=\"english\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424481a-bea3-49c8-8b3f-a4ec62220437",
   "metadata": {},
   "source": [
    "In this step, I will computes the tfidf representation of the documents, retrieves the feature names, and displays the tfidf vector for the first document in array format,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a19246-4dbe-4b79-a9e4-81753bd10ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document vectors\n",
    "tfidf_ft_matrix = tfidf_vectorizer.fit_transform(documents.values())\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the vector for the first document\n",
    "tfidf_ft_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aac45c-0921-439b-b8ae-bd8af7a4eac0",
   "metadata": {},
   "source": [
    "In this step, I will initializes and fits an NMF model to the tfidf matrix, resulting in document-topic and topic-term matrices, These matrices allow you to analyze the distribution of topics across documents and the distribution of terms within topics, which are crucial for interpreting the results of topic modeling. I used nmf because it generates non-negative topic-term and document-topic matrices which means more interpretable topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1a639-0435-4c7d-9cdc-1cb734c0fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of topics\n",
    "num_topics = 15\n",
    "\n",
    "# Create the model\n",
    "nmf_model = NMF(n_components=num_topics,init='random',beta_loss='frobenius', max_iter=500, random_state=42)\n",
    "\n",
    "# Fit the model to the data and use it to transform the data\n",
    "doc_topic_nmf = nmf_model.fit_transform(tfidf_ft_matrix)\n",
    "\n",
    "topic_term_nmf = nmf_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2db6a7-2333-4f6b-87ed-a868ebd1ce0f",
   "metadata": {},
   "source": [
    "In this step, I will store all the topics and their terms in the dictionary and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801d981-4d55-4fa5-ad2a-07106cc5a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their terms\n",
    "nmf_topic_dict = {}\n",
    "for index, topic in enumerate(topic_term_nmf):\n",
    "    zipped = zip(feature_names, topic)\n",
    "    top_terms=dict(sorted(zipped, key = lambda t: t[1], reverse=True)[:10])\n",
    "    #print(top_terms)\n",
    "    top_terms_list= {key : round(top_terms[key], 4) for key in top_terms.keys()}\n",
    "    nmf_topic_dict[f\"topic_{index}\"] = top_terms_list\n",
    "\n",
    "# Print the topics with their terms    \n",
    "for k,v in nmf_topic_dict.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6067f-14ca-4270-8b66-df2570f7fe82",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this step, I would like to label the topics produced above according to their terms and weights for better understanding and the identification of topics specific to AI risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46b5f9-62b1-4e39-ab0a-5293ac1f06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and label each topic\n",
    "topic_labels = {}\n",
    "\n",
    "# Example labels based on provided topics and their top terms\n",
    "topic_labels[0] = \"Australian Politics\"\n",
    "topic_labels[1] = \"Indigenous America\"  \n",
    "topic_labels[2] = \"Epstein Cases\" \n",
    "topic_labels[3] = \"Australian News\" \n",
    "topic_labels[4] = \"Cybersecurity\"\n",
    "topic_labels[5] = \"Deception and Privacy(AI)\" \n",
    "topic_labels[6] = \"Identification and Surveillance(AI)\"\n",
    "topic_labels[7] = \"Disinformation(AI)\" \n",
    "topic_labels[8] = \"Market Indicators\"\n",
    "topic_labels[9] = \"Safety and Regulations(AI)\"\n",
    "topic_labels[10] = \"Online Harms and legal Issues\"\n",
    "topic_labels[11] = \"Market Competition\"\n",
    "topic_labels[12] = \"Political Conspiracies\" \n",
    "topic_labels[13] = \"Global Economic Forum (Davos)\" \n",
    "topic_labels[14] = \"Gaza and Israel War\"\n",
    "\n",
    "# Verify the topic labels\n",
    "for index, label in topic_labels.items():\n",
    "    print(f\"Topic {index}: {label}\")\n",
    "    print(f\"Top terms: {nmf_topic_dict[f'topic_{index}']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbd9c8-1e64-425e-9333-91d8918c3719",
   "metadata": {},
   "source": [
    "## VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53a15f-7a06-4147-82cf-bae720e9a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code has been generated with the help of Chatgpt\n",
    "\n",
    "# Calculate topic distribution (sum of topic weights across all documents)\n",
    "topic_distribution = doc_topic_nmf.sum(axis=0)\n",
    "\n",
    "# Update topic_labels_list based on the updated topic_labels dictionary\n",
    "topic_labels_list = [topic_labels[i] for i in range(num_topics)]\n",
    "\n",
    "# Convert topic distribution and labels to a DataFrame\n",
    "topic_distribution_df = pd.DataFrame({'Topics': topic_labels_list, 'Document Count': topic_distribution})\n",
    "\n",
    "# Create a custom color palette for the bars\n",
    "custom_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "                 '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
    "                 '#ff0000', '#00ff00', '#0000ff', '#ffff00', '#00ffff']\n",
    "\n",
    "# Create a bar plot using Plotly Express with custom colors\n",
    "fig = px.bar(topic_distribution_df, x='Topics', y='Document Count', \n",
    "             title='Topic Distribution',\n",
    "             labels={'Document Count': 'Document Count', 'Topics': 'Topics'},\n",
    "             color='Document Count',\n",
    "             color_continuous_scale=custom_colors)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "fig.update_layout(xaxis=dict(tickangle=45))\n",
    "\n",
    "# Increase the height of the graph\n",
    "fig.update_layout(height=600)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1acf7-cfea-45b2-a665-4f1fdea60f12",
   "metadata": {},
   "source": [
    "## INSIGHTS:\n",
    "The chart visualizes the relationship between topics and their document counts within the text corpus.\n",
    "\n",
    "X-axis (Topics): Each bar on the chart represents a topic derived from the text corpus. The topics are labeled based on their content or themes, as assigned in the topic_labels dictionary.\n",
    "\n",
    "Y-axis (Document Count): The height of each bar represents the total number of documents in the corpus associated with the corresponding topic. It indicates the prevalence or frequency of each topic within the corpus.\n",
    "\n",
    "To conclude, leveraging topic modeling, tfidf, and nmf methods, I've identified four significant risks or challenges facing users, regulators, and lawmakers in the context of AI in the near future:\n",
    "\n",
    "Deception and Privacy Risk, \n",
    "Identification and Surveillance Risk, \n",
    "Safety and Regulations Risk, \n",
    "Disinformation Risk.\n",
    "\n",
    "The higher document count associated with the safety and regulations risk underscores the urgent need for swift and comprehensive directives aimed at addressing safety concerns and establishing robust regulatory frameworks to mitigate potential risks associated with AI technologies.\n",
    "\n",
    "## Ethical Consideration: Unawareness and Lack of Capacity:\n",
    "\n",
    "My analysis that swift and comprehensive directives aimed at addressing safety concerns and establishing robust regulatory frameworks to mitigate potential risks associated with AI technologies may be weak on ethical ground because the regulators and law makers may not have the technical background necessary to fully comprehend the complexities of AI technology. This gap in understanding can lead to misinformed decisions, inadequate regulations, and an inability to anticipate or mitigate risks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafd73c-1954-44af-aff1-e02bef628c19",
   "metadata": {},
   "source": [
    "## FURTHER ANALYSIS USING MACHINE LEARNING TECHNIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6ae85-840f-421e-9506-8d9ae776b4b1",
   "metadata": {},
   "source": [
    "In this step, I would like to group documents into clusters based on their topic distributions using Kmeans machine learning technique and then determine and assign a dominant topic to each cluster which would again emphasise the importance of topics within documents. Further, it will also organize and identify common themes among documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1751a2f-f79e-43ae-84ad-dcd61a47dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KMeans clustering model\n",
    "num_clusters = num_topics  # Using the number of topics as the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(doc_topic_nmf)\n",
    "\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db619a-156f-4142-b94d-278d4bd09f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code has been generated with the help of Chatgpt\n",
    "\n",
    "# Group documents by cluster\n",
    "clusters = {i: [] for i in range(num_clusters)}\n",
    "for doc_index, cluster_label in enumerate(cluster_labels):\n",
    "    clusters[cluster_label].append(doc_index)\n",
    "\n",
    "# Determine the dominant topic in each cluster\n",
    "cluster_topic_distribution = np.zeros((num_clusters, num_topics))\n",
    "\n",
    "for cluster_label, doc_indices in clusters.items():\n",
    "    for doc_index in doc_indices:\n",
    "        cluster_topic_distribution[cluster_label] += doc_topic_nmf[doc_index]\n",
    "\n",
    "# Assign cluster names based on the dominant topics\n",
    "cluster_names = {}\n",
    "for cluster_label in range(num_clusters):\n",
    "    dominant_topic = np.argmax(cluster_topic_distribution[cluster_label])\n",
    "    cluster_names[cluster_label] = topic_labels[dominant_topic]\n",
    "\n",
    "# Print the cluster name and documents in each cluster\n",
    "for cluster_label, doc_indices in clusters.items():\n",
    "    print(f\"Cluster {cluster_label} ({cluster_names[cluster_label]}): {doc_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a569b19-89e9-4250-872c-c272ffb23328",
   "metadata": {},
   "source": [
    "## INSIGHTS:\n",
    "\n",
    "The above output illustrate the following insights.\n",
    "\n",
    "1. Clustering Documents: By using KMeans clustering, the code groups documents that have similar topic distributions (as identified by NMF).\n",
    "2. Determining Dominant Topics: For each cluster, it calculates the overall topic distribution and identifies the most prevalent topic within the cluster.\n",
    "3. Assigning Cluster Names: Based on the dominant topic, it assigns a descriptive name to each cluster.\n",
    "4. Visualizing Results: Finally, it prints out the cluster names and the documents belonging to each cluster, which helps in understanding the composition of each cluster in terms of topic distribution.\n",
    "\n",
    "## Ethical Point of View: Verification and Validation:\n",
    "\n",
    "My limited understanding of K-means means I may not fully grasp the nuances and potential pitfalls of the algorithm, such as how it handles different types of data or the influence of initial centroids. This can lead to weak reasoning and conclusions based on the algorithm's output, undermining the credibility of my analysis. It becomes difficult to ensure that the clusters identified are meaningful and accurately represent the underlying data patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac74dc-a4b4-47aa-b64e-ef0dde401a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creation_period": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nb_name": "template",
  "qut": {
   "creation_period": "2023_sem1",
   "nb_name": "template-assignment2",
   "unit_code": "IFN619"
  },
  "unit_code": "",
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
